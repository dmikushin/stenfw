/**
 * Kernel and initialization code for the stencil
 *
 * (c1[0][0]=(2.0-(dt_dx_sq*7.5)))
 * (c2[0][0]=(dt_dx_sq*1.3333333333333333))
 * (c3[0][0]=(dt_dx_sq*-0.08333333333333333))
 * (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0])))))))
 * 
 * Strategy: C:\Users\Matthias\Projects\Patus\strategy\cacheblocked.stg
 * 
 * This code was generated by Patus on 2012/04/12 22:50:06
 */

#include "immintrin.h"
#include "x86intrin.h"
#include "wave13pt_patus.h"

#include <omp.h>
#include <stdint.h>
#include <stdio.h>

int max (int a, int b)
{
	return (a > b) ? a : b;
}

int min (int a, int b)
{
	return (a < b) ? a : b;
}

static void kernel(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy111;
	int64_t dummy112;
	int64_t dummy113;
	int64_t dummy114;
	int64_t dummy115;
	int64_t dummy116;
	int64_t dummy117;
	int64_t dummy118;
	int64_t dummy119;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
//printf ("wave___unroll_p3_20201\n");

	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy111), "=&r"(dummy112), "=&r"(dummy113)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy114), "=&r"(dummy115), "=&r"(dummy116)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy117), "=&r"(dummy118), "=&r"(dummy119)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_40102(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy39;
	int64_t dummy40;
	int64_t dummy41;
	int64_t dummy42;
	int64_t dummy43;
	int64_t dummy44;
	int64_t dummy45;
	int64_t dummy46;
	int64_t dummy47;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<(v2_idx_z_max-1); p3_idx_z+=2)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy39), "=&r"(dummy40), "=&r"(dummy41)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx1=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy42), "=&r"(dummy43), "=&r"(dummy44)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
				for (; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy45), "=&r"(dummy46), "=&r"(dummy47)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_40101(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy36;
	int64_t dummy37;
	int64_t dummy38;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy36), "=&r"(dummy37), "=&r"(dummy38)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_20101(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy100;
	int64_t dummy101;
	int64_t dummy99;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy99), "=&r"(dummy100), "=&r"(dummy101)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_20202(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	int _idx2;
	int _idx3;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy120;
	int64_t dummy121;
	int64_t dummy122;
	int64_t dummy123;
	int64_t dummy124;
	int64_t dummy125;
	int64_t dummy126;
	int64_t dummy127;
	int64_t dummy128;
	int64_t dummy129;
	int64_t dummy130;
	int64_t dummy131;
	int64_t dummy132;
	int64_t dummy133;
	int64_t dummy134;
	int64_t dummy135;
	int64_t dummy136;
	int64_t dummy137;
	int64_t dummy138;
	int64_t dummy139;
	int64_t dummy140;
	int64_t dummy141;
	int64_t dummy142;
	int64_t dummy143;
	int64_t dummy144;
	int64_t dummy145;
	int64_t dummy146;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<(v2_idx_z_max-1); p3_idx_z+=2)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy120), "=&r"(dummy121), "=&r"(dummy122)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy123), "=&r"(dummy124), "=&r"(dummy125)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx2 = ((x_max*((y_max*(p3_idx_z+1))+(p3_idx_y+1)))+p3_idx_x) */
						_idx2=(_idx1+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy126), "=&r"(dummy127), "=&r"(dummy128)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx2]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx2]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx2]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx3 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx3=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy129), "=&r"(dummy130), "=&r"(dummy131)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx3]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx3]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx3]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy132), "=&r"(dummy133), "=&r"(dummy134)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx1=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy135), "=&r"(dummy136), "=&r"(dummy137)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
				for (; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy138), "=&r"(dummy139), "=&r"(dummy140)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy141), "=&r"(dummy142), "=&r"(dummy143)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy144), "=&r"(dummy145), "=&r"(dummy146)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_20102(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy102;
	int64_t dummy103;
	int64_t dummy104;
	int64_t dummy105;
	int64_t dummy106;
	int64_t dummy107;
	int64_t dummy108;
	int64_t dummy109;
	int64_t dummy110;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<(v2_idx_z_max-1); p3_idx_z+=2)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy102), "=&r"(dummy103), "=&r"(dummy104)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx1=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy105), "=&r"(dummy106), "=&r"(dummy107)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
				for (; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $4, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm3\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm2\n\t"
						"vaddps -4(%1), %%ymm3, %%ymm1\n\t"
						"vaddps 28(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm2\n\t"
						"vmovups 8(%1), %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm4\n\t"
						"vaddps -8(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 24(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vaddps 32(%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm2, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm2, %%ymm5, %%ymm2\n\t"
						"vmovaps (%10), %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm2, %%ymm0\n\t"
						"vmulps (%1), %%ymm4, %%ymm2\n\t"
						"vmulps 32(%1), %%ymm4, %%ymm4\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vsubps 32(%0), %%ymm4, %%ymm4\n\t"
						"vaddps %%ymm2, %%ymm1, %%ymm2\n\t"
						"vaddps %%ymm4, %%ymm0, %%ymm4\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"vmovaps %%ymm4, 32(%2)\n\t"
						"addq $64, %0\n\t"
						"addq $64, %1\n\t"
						"addq $64, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $4, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy108), "=&r"(dummy109), "=&r"(dummy110)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_40201(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy0;
	int64_t dummy1;
	int64_t dummy2;
	int64_t dummy3;
	int64_t dummy4;
	int64_t dummy5;
	int64_t dummy6;
	int64_t dummy7;
	int64_t dummy8;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy0), "=&r"(dummy1), "=&r"(dummy2)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy3), "=&r"(dummy4), "=&r"(dummy5)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy6), "=&r"(dummy7), "=&r"(dummy8)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_10202(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	int _idx2;
	int _idx3;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy69;
	int64_t dummy70;
	int64_t dummy71;
	int64_t dummy72;
	int64_t dummy73;
	int64_t dummy74;
	int64_t dummy75;
	int64_t dummy76;
	int64_t dummy77;
	int64_t dummy78;
	int64_t dummy79;
	int64_t dummy80;
	int64_t dummy81;
	int64_t dummy82;
	int64_t dummy83;
	int64_t dummy84;
	int64_t dummy85;
	int64_t dummy86;
	int64_t dummy87;
	int64_t dummy88;
	int64_t dummy89;
	int64_t dummy90;
	int64_t dummy91;
	int64_t dummy92;
	int64_t dummy93;
	int64_t dummy94;
	int64_t dummy95;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<(v2_idx_z_max-1); p3_idx_z+=2)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy69), "=&r"(dummy70), "=&r"(dummy71)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy72), "=&r"(dummy73), "=&r"(dummy74)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx2 = ((x_max*((y_max*(p3_idx_z+1))+(p3_idx_y+1)))+p3_idx_x) */
						_idx2=(_idx1+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy75), "=&r"(dummy76), "=&r"(dummy77)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx2]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx2]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx2]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx3 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx3=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy78), "=&r"(dummy79), "=&r"(dummy80)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx3]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx3]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx3]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy81), "=&r"(dummy82), "=&r"(dummy83)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx1=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy84), "=&r"(dummy85), "=&r"(dummy86)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
				for (; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy87), "=&r"(dummy88), "=&r"(dummy89)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy90), "=&r"(dummy91), "=&r"(dummy92)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy93), "=&r"(dummy94), "=&r"(dummy95)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_80101(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy96;
	int64_t dummy97;
	int64_t dummy98;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $6, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm1\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm6\n\t"
						"vmovups 68(%1), %%ymm2\n\t"
						"vaddps 28(%1), %%ymm1, %%ymm7\n\t"
						"vmovups 100(%1), %%ymm3\n\t"
						"vaddps 60(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 132(%1), %%ymm4\n\t"
						"vaddps 92(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 164(%1), %%ymm5\n\t"
						"vaddps 124(%1), %%ymm4, %%ymm3\n\t"
						"vmovups 196(%1), %%ymm8\n\t"
						"vaddps 156(%1), %%ymm5, %%ymm4\n\t"
						"vmovups 228(%1), %%ymm10\n\t"
						"vaddps 188(%1), %%ymm8, %%ymm5\n\t"
						"vaddps 220(%1), %%ymm10, %%ymm9\n\t"
						"vaddps (%1,%7), %%ymm6, %%ymm6\n\t"
						"vaddps 32(%1,%7), %%ymm7, %%ymm7\n\t"
						"vaddps 64(%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 96(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 128(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 160(%1,%7), %%ymm4, %%ymm4\n\t"
						"vaddps 192(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps 224(%1,%7), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9), %%ymm6, %%ymm6\n\t"
						"vaddps 32(%1,%9), %%ymm7, %%ymm7\n\t"
						"vaddps 64(%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 96(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 128(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 160(%1,%9), %%ymm4, %%ymm4\n\t"
						"vaddps 192(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps 224(%1,%9), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8), %%ymm6, %%ymm6\n\t"
						"vaddps 32(%1,%8), %%ymm7, %%ymm7\n\t"
						"vaddps 64(%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 96(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 128(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 160(%1,%8), %%ymm4, %%ymm4\n\t"
						"vaddps 192(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps 224(%1,%8), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6), %%ymm6, %%ymm6\n\t"
						"vaddps 32(%1,%6), %%ymm7, %%ymm7\n\t"
						"vaddps 64(%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 96(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 128(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 160(%1,%6), %%ymm4, %%ymm4\n\t"
						"vaddps 192(%1,%6), %%ymm5, %%ymm5\n\t"
						"vaddps 224(%1,%6), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm0, %%ymm6, %%ymm6\n\t"
						"vmovaps %%ymm6, 96(%10)\n\t"
						"vmulps %%ymm0, %%ymm7, %%ymm7\n\t"
						"vmovaps %%ymm7, 128(%10)\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm4, %%ymm4\n\t"
						"vmovaps 64(%10), %%ymm8\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm9, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm11\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm7\n\t"
						"vmovups 72(%1), %%ymm6\n\t"
						"vaddps 24(%1), %%ymm11, %%ymm9\n\t"
						"vmovups 104(%1), %%ymm13\n\t"
						"vaddps 56(%1), %%ymm6, %%ymm10\n\t"
						"vmovups 136(%1), %%ymm6\n\t"
						"vaddps 88(%1), %%ymm13, %%ymm11\n\t"
						"vmovups 168(%1), %%ymm15\n\t"
						"vaddps 120(%1), %%ymm6, %%ymm12\n\t"
						"vmovups 200(%1), %%ymm6\n\t"
						"vaddps 152(%1), %%ymm15, %%ymm13\n\t"
						"vmovups 232(%1), %%ymm15\n\t"
						"vaddps 184(%1), %%ymm6, %%ymm14\n\t"
						"vaddps 216(%1), %%ymm15, %%ymm6\n\t"
						"vaddps (%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 32(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps 64(%1,%7,2), %%ymm10, %%ymm10\n\t"
						"vaddps 96(%1,%7,2), %%ymm11, %%ymm11\n\t"
						"vaddps 128(%1,%7,2), %%ymm12, %%ymm12\n\t"
						"vaddps 160(%1,%7,2), %%ymm13, %%ymm13\n\t"
						"vaddps 192(%1,%7,2), %%ymm14, %%ymm14\n\t"
						"vaddps 224(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps (%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 32(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps 64(%1,%9,2), %%ymm10, %%ymm10\n\t"
						"vaddps 96(%1,%9,2), %%ymm11, %%ymm11\n\t"
						"vaddps 128(%1,%9,2), %%ymm12, %%ymm12\n\t"
						"vaddps 160(%1,%9,2), %%ymm13, %%ymm13\n\t"
						"vaddps 192(%1,%9,2), %%ymm14, %%ymm14\n\t"
						"vaddps 224(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps (%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 32(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps 64(%1,%8,2), %%ymm10, %%ymm10\n\t"
						"vaddps 96(%1,%8,2), %%ymm11, %%ymm11\n\t"
						"vaddps 128(%1,%8,2), %%ymm12, %%ymm12\n\t"
						"vaddps 160(%1,%8,2), %%ymm13, %%ymm13\n\t"
						"vaddps 192(%1,%8,2), %%ymm14, %%ymm14\n\t"
						"vaddps 224(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps (%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 32(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vaddps 64(%1,%6,2), %%ymm10, %%ymm10\n\t"
						"vaddps 96(%1,%6,2), %%ymm11, %%ymm11\n\t"
						"vaddps 128(%1,%6,2), %%ymm12, %%ymm12\n\t"
						"vaddps 160(%1,%6,2), %%ymm13, %%ymm13\n\t"
						"vaddps 192(%1,%6,2), %%ymm14, %%ymm14\n\t"
						"vaddps 224(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm8, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm8, %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm8, %%ymm10, %%ymm10\n\t"
						"vmulps %%ymm8, %%ymm11, %%ymm11\n\t"
						"vmulps %%ymm8, %%ymm12, %%ymm12\n\t"
						"vmulps %%ymm8, %%ymm13, %%ymm13\n\t"
						"vmulps %%ymm8, %%ymm14, %%ymm14\n\t"
						"vmulps %%ymm8, %%ymm6, %%ymm8\n\t"
						"vmovaps 96(%10), %%ymm6\n\t"
						"vaddps %%ymm6, %%ymm7, %%ymm6\n\t"
						"vmovaps 128(%10), %%ymm7\n\t"
						"vaddps %%ymm7, %%ymm9, %%ymm7\n\t"
						"vaddps %%ymm1, %%ymm10, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm11, %%ymm2\n\t"
						"vaddps %%ymm3, %%ymm12, %%ymm3\n\t"
						"vaddps %%ymm4, %%ymm13, %%ymm4\n\t"
						"vmovaps (%10), %%ymm9\n\t"
						"vaddps %%ymm5, %%ymm14, %%ymm5\n\t"
						"vaddps %%ymm0, %%ymm8, %%ymm0\n\t"
						"vmulps (%1), %%ymm9, %%ymm13\n\t"
						"vmulps 32(%1), %%ymm9, %%ymm11\n\t"
						"vmulps 64(%1), %%ymm9, %%ymm12\n\t"
						"vmulps 96(%1), %%ymm9, %%ymm8\n\t"
						"vmulps 128(%1), %%ymm9, %%ymm10\n\t"
						"vmulps 160(%1), %%ymm9, %%ymm14\n\t"
						"vmulps 192(%1), %%ymm9, %%ymm15\n\t"
						"vmulps 224(%1), %%ymm9, %%ymm9\n\t"
						"vsubps (%0), %%ymm13, %%ymm13\n\t"
						"vsubps 32(%0), %%ymm11, %%ymm11\n\t"
						"vsubps 64(%0), %%ymm12, %%ymm12\n\t"
						"vsubps 96(%0), %%ymm8, %%ymm8\n\t"
						"vsubps 128(%0), %%ymm10, %%ymm10\n\t"
						"vsubps 160(%0), %%ymm14, %%ymm14\n\t"
						"vsubps 192(%0), %%ymm15, %%ymm15\n\t"
						"vsubps 224(%0), %%ymm9, %%ymm9\n\t"
						"vaddps %%ymm13, %%ymm6, %%ymm13\n\t"
						"vaddps %%ymm11, %%ymm7, %%ymm11\n\t"
						"vaddps %%ymm12, %%ymm1, %%ymm12\n\t"
						"vaddps %%ymm8, %%ymm2, %%ymm8\n\t"
						"vaddps %%ymm10, %%ymm3, %%ymm10\n\t"
						"vaddps %%ymm14, %%ymm4, %%ymm14\n\t"
						"vaddps %%ymm15, %%ymm5, %%ymm15\n\t"
						"vaddps %%ymm9, %%ymm0, %%ymm9\n\t"
						"vmovaps %%ymm13, (%2)\n\t"
						"vmovaps %%ymm11, 32(%2)\n\t"
						"vmovaps %%ymm12, 64(%2)\n\t"
						"vmovaps %%ymm8, 96(%2)\n\t"
						"vmovaps %%ymm10, 128(%2)\n\t"
						"vmovaps %%ymm14, 160(%2)\n\t"
						"vmovaps %%ymm15, 192(%2)\n\t"
						"vmovaps %%ymm9, 224(%2)\n\t"
						"addq $256, %0\n\t"
						"addq $256, %1\n\t"
						"addq $256, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $6, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy96), "=&r"(dummy97), "=&r"(dummy98)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14", "xmm15", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_10102(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy51;
	int64_t dummy52;
	int64_t dummy53;
	int64_t dummy54;
	int64_t dummy55;
	int64_t dummy56;
	int64_t dummy57;
	int64_t dummy58;
	int64_t dummy59;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<(v2_idx_z_max-1); p3_idx_z+=2)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy51), "=&r"(dummy52), "=&r"(dummy53)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx1=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy54), "=&r"(dummy55), "=&r"(dummy56)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
				for (; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy57), "=&r"(dummy58), "=&r"(dummy59)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_10201(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy60;
	int64_t dummy61;
	int64_t dummy62;
	int64_t dummy63;
	int64_t dummy64;
	int64_t dummy65;
	int64_t dummy66;
	int64_t dummy67;
	int64_t dummy68;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy60), "=&r"(dummy61), "=&r"(dummy62)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy63), "=&r"(dummy64), "=&r"(dummy65)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy66), "=&r"(dummy67), "=&r"(dummy68)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_10101(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy48;
	int64_t dummy49;
	int64_t dummy50;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* unaligned epilog */
						"2:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 4f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"4:\n\t"
						: "=&r"(dummy48), "=&r"(dummy49), "=&r"(dummy50)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

void wave___unroll_p3_40202(float *  *  u_0_1_out, float *  u_0_m1, float *  u_0_0, float *  u_0_1, float dt_dx_sq, int x_max, int y_max, int z_max, int cb_x, int cb_y, int cb_z, int chunk)
{
	int _idx0;
	int _idx1;
	int _idx2;
	int _idx3;
	float const0 = (2.0f-(dt_dx_sq*7.5f));
	float const1 = (dt_dx_sq*1.3333333333333333f);
	float const2 = (dt_dx_sq*-0.08333333333333333f);
	__m256 constarr0[] =  {  { const0, const0, const0, const0, const0, const0, const0, const0 } ,  { const1, const1, const1, const1, const1, const1, const1, const1 } ,  { const2, const2, const2, const2, const2, const2, const2, const2 } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f } ,  { 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f }  } ;
	int64_t dummy10;
	int64_t dummy11;
	int64_t dummy12;
	int64_t dummy13;
	int64_t dummy14;
	int64_t dummy15;
	int64_t dummy16;
	int64_t dummy17;
	int64_t dummy18;
	int64_t dummy19;
	int64_t dummy20;
	int64_t dummy21;
	int64_t dummy22;
	int64_t dummy23;
	int64_t dummy24;
	int64_t dummy25;
	int64_t dummy26;
	int64_t dummy27;
	int64_t dummy28;
	int64_t dummy29;
	int64_t dummy30;
	int64_t dummy31;
	int64_t dummy32;
	int64_t dummy33;
	int64_t dummy34;
	int64_t dummy35;
	int64_t dummy9;
	int end0;
	int numthds0;
	int p3_idx_x;
	int p3_idx_y;
	int p3_idx_z;
	int start0;
	int stepouter0;
	int tmp_stride_0z;
	int tmpidxc0;
	int v2_blkidx_x;
	int v2_blkidx_x_idxouter;
	int v2_idx_x;
	int v2_idx_x_max;
	int v2_idx_y;
	int v2_idx_y_max;
	int v2_idx_z;
	int v2_idx_z_max;
	/*
	Implementation
	*/
	start0=omp_get_thread_num();
	end0=(((((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)))*((int)(((z_max+cb_z)-5)/cb_z)))-1);
	numthds0=omp_get_num_threads();
	stepouter0=(chunk*numthds0);
	/*
	for v2_blkidx_x_idxouter = (start0*chunk)..end0 by stepouter0 parallel 1 <level 1> schedule 1 { ... }
	*/
	for (v2_blkidx_x_idxouter=(start0*chunk); v2_blkidx_x_idxouter<=end0; v2_blkidx_x_idxouter+=stepouter0)
	{
		/*
		for v2_blkidx_x = v2_blkidx_x_idxouter..min(end0, ((v2_blkidx_x_idxouter+chunk)-1)) by 1 parallel 1 <level 1> schedule 1 { ... }
		*/
		for (v2_blkidx_x=v2_blkidx_x_idxouter; v2_blkidx_x<=min(end0, ((v2_blkidx_x_idxouter+chunk)-1)); v2_blkidx_x+=1)
		{
			tmp_stride_0z=(((int)(((x_max+cb_x)-5)/cb_x))*((int)(((y_max+cb_y)-5)/cb_y)));
			v2_idx_z=(v2_blkidx_x/tmp_stride_0z);
			tmpidxc0=(v2_blkidx_x-(v2_idx_z*tmp_stride_0z));
			v2_idx_y=(tmpidxc0/((int)(((x_max+cb_x)-5)/cb_x)));
			tmpidxc0-=(v2_idx_y*((int)(((x_max+cb_x)-5)/cb_x)));
			v2_idx_x=tmpidxc0;
			v2_idx_x=((v2_idx_x*cb_x)+2);
			v2_idx_x_max=min((v2_idx_x+cb_x), ((x_max-3)+1));
			v2_idx_y=((v2_idx_y*cb_y)+2);
			v2_idx_y_max=min((v2_idx_y+cb_y), ((y_max-3)+1));
			v2_idx_z=((v2_idx_z*cb_z)+2);
			v2_idx_z_max=min((v2_idx_z+cb_z), ((z_max-3)+1));
			/* Index bounds calculations for iterators in v2[t=t][0] */
			/*
			for POINT p3[t=t][0] of size [1, 1, 1] in v2[t=t][0] + [ min=[0, 0, 0], max=[0, 0, 0] ] parallel 1 <level 1> schedule default { ... }
			*/
			{
				/* Index bounds calculations for iterators in p3[t=t][0] */
				for (p3_idx_z=v2_idx_z; p3_idx_z<(v2_idx_z_max-1); p3_idx_z+=2)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy9), "=&r"(dummy10), "=&r"(dummy11)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy12), "=&r"(dummy13), "=&r"(dummy14)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx2 = ((x_max*((y_max*(p3_idx_z+1))+(p3_idx_y+1)))+p3_idx_x) */
						_idx2=(_idx1+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy15), "=&r"(dummy16), "=&r"(dummy17)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx2]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx2]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx2]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx3 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx3=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy18), "=&r"(dummy19), "=&r"(dummy20)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx3]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx3]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx3]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy21), "=&r"(dummy22), "=&r"(dummy23)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*(p3_idx_z+1))+p3_idx_y))+p3_idx_x) */
						_idx1=(_idx0+(x_max*y_max));
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy24), "=&r"(dummy25), "=&r"(dummy26)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
				for (; p3_idx_z<v2_idx_z_max; p3_idx_z+=1)
				{
					for (p3_idx_y=v2_idx_y; p3_idx_y<(v2_idx_y_max-1); p3_idx_y+=2)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy27), "=&r"(dummy28), "=&r"(dummy29)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
						/* _idx1 = ((x_max*((y_max*p3_idx_z)+(p3_idx_y+1)))+p3_idx_x) */
						_idx1=(_idx0+x_max);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy30), "=&r"(dummy31), "=&r"(dummy32)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx1]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx1]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx1]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
					for (; p3_idx_y<v2_idx_y_max; p3_idx_y+=1)
					{
						p3_idx_x=v2_idx_x;
						/* _idx0 = ((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x) */
						_idx0=((x_max*((y_max*p3_idx_z)+p3_idx_y))+p3_idx_x);
						__asm__ __volatile__ (
						/* unaligned prolog */
						"mov %2, %%rax\n\t"
						"add $31, %%rax\n\t"
						"and $31, %%rax\n\t"
						"sub $32, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $2, %%rax\n\t"
						"cmp %%rax, %11\n\t"
						"cmovng %11, %%rax\n\t"
						"mov %%rax, %%rbx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 1f\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"shl $2, %%rax\n\t"
						"addq %%rax, %0\n\t"
						"addq %%rax, %1\n\t"
						"addq %%rax, %2\n\t"
						/* (unrolled) aligned main loop */
						"mov %%rbx, %%rax\n\t"
						"1:\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $5, %%rax\n\t"
						"mov %%rax, %%rcx\n\t"
						"or %%rax, %%rax\n\t"
						"jz 2f\n\t"
						"3:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm2\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vmovups 36(%1), %%ymm3\n\t"
						"vaddps -4(%1), %%ymm2, %%ymm1\n\t"
						"vmovups 68(%1), %%ymm5\n\t"
						"vaddps 28(%1), %%ymm3, %%ymm2\n\t"
						"vmovups 100(%1), %%ymm4\n\t"
						"vaddps 60(%1), %%ymm5, %%ymm3\n\t"
						"vaddps 92(%1), %%ymm4, %%ymm5\n\t"
						"vaddps (%1,%7), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%7), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%7), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%9), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%9), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%9), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%8), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%8), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%8), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%8), %%ymm5, %%ymm5\n\t"
						"vaddps (%1,%6), %%ymm1, %%ymm1\n\t"
						"vaddps 32(%1,%6), %%ymm2, %%ymm2\n\t"
						"vaddps 64(%1,%6), %%ymm3, %%ymm3\n\t"
						"vaddps 96(%1,%6), %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm0, %%ymm1, %%ymm1\n\t"
						"vmulps %%ymm0, %%ymm2, %%ymm2\n\t"
						"vmovaps 64(%10), %%ymm4\n\t"
						"vmovups 8(%1), %%ymm6\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm5, %%ymm0\n\t"
						"vmovups 40(%1), %%ymm7\n\t"
						"vaddps -8(%1), %%ymm6, %%ymm5\n\t"
						"vmovups 72(%1), %%ymm9\n\t"
						"vaddps 24(%1), %%ymm7, %%ymm6\n\t"
						"vmovups 104(%1), %%ymm8\n\t"
						"vaddps 56(%1), %%ymm9, %%ymm7\n\t"
						"vaddps 88(%1), %%ymm8, %%ymm9\n\t"
						"vaddps (%1,%7,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%7,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%7,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%7,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%9,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%9,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%9,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%9,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%8,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%8,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%8,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%8,2), %%ymm9, %%ymm9\n\t"
						"vaddps (%1,%6,2), %%ymm5, %%ymm5\n\t"
						"vaddps 32(%1,%6,2), %%ymm6, %%ymm6\n\t"
						"vaddps 64(%1,%6,2), %%ymm7, %%ymm7\n\t"
						"vaddps 96(%1,%6,2), %%ymm9, %%ymm9\n\t"
						"vmulps %%ymm4, %%ymm5, %%ymm5\n\t"
						"vmulps %%ymm4, %%ymm6, %%ymm6\n\t"
						"vmulps %%ymm4, %%ymm7, %%ymm7\n\t"
						"vmulps %%ymm4, %%ymm9, %%ymm4\n\t"
						"vaddps %%ymm1, %%ymm5, %%ymm1\n\t"
						"vaddps %%ymm2, %%ymm6, %%ymm2\n\t"
						"vmovaps (%10), %%ymm5\n\t"
						"vaddps %%ymm3, %%ymm7, %%ymm3\n\t"
						"vaddps %%ymm0, %%ymm4, %%ymm0\n\t"
						"vmulps (%1), %%ymm5, %%ymm4\n\t"
						"vmulps 32(%1), %%ymm5, %%ymm7\n\t"
						"vmulps 64(%1), %%ymm5, %%ymm6\n\t"
						"vmulps 96(%1), %%ymm5, %%ymm5\n\t"
						"vsubps (%0), %%ymm4, %%ymm4\n\t"
						"vsubps 32(%0), %%ymm7, %%ymm7\n\t"
						"vsubps 64(%0), %%ymm6, %%ymm6\n\t"
						"vsubps 96(%0), %%ymm5, %%ymm5\n\t"
						"vaddps %%ymm4, %%ymm1, %%ymm4\n\t"
						"vaddps %%ymm7, %%ymm2, %%ymm7\n\t"
						"vaddps %%ymm6, %%ymm3, %%ymm6\n\t"
						"vaddps %%ymm5, %%ymm0, %%ymm5\n\t"
						"vmovaps %%ymm4, (%2)\n\t"
						"vmovaps %%ymm7, 32(%2)\n\t"
						"vmovaps %%ymm6, 64(%2)\n\t"
						"vmovaps %%ymm5, 96(%2)\n\t"
						"addq $128, %0\n\t"
						"addq $128, %1\n\t"
						"addq $128, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 3b\n\t"
						/* aligned unrolling cleanup loop */
						"2:\n\t"
						"mov %%rcx, %%rax\n\t"
						"shl $5, %%rax\n\t"
						"add %%rbx, %%rax\n\t"
						"sub %11, %%rax\n\t"
						"neg %%rax\n\t"
						"shr $3, %%rax\n\t"
						"or %%rax, %%rax\n\t"
						"jz 4f\n\t"
						"5:\n\t"
						".align 4 \n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovaps 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovaps 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovaps (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovaps %%ymm2, (%2)\n\t"
						"addq $32, %0\n\t"
						"addq $32, %1\n\t"
						"addq $32, %2\n\t"
						"sub $1, %%rax\n\t"
						"jnz 5b\n\t"
						/* unaligned epilog */
						"4:\n\t"
						"sub %11, %%rbx\n\t"
						"and $7, %%rbx\n\t"
						"or %%rbx, %%rbx\n\t"
						"jz 6f\n\t"
						"shl $2, %%rbx\n\t"
						"sub %%rbx, %0\n\t"
						"sub %%rbx, %1\n\t"
						"sub %%rbx, %2\n\t"
						"vmovups 4(%1), %%ymm1\n\t"
						/* (u[0][0][0][1][0]=(((c1*u[0][0][0][0][0])-u[0][0][0][-1][0])+((c2*((u[1][0][0][0][0]+(u[-1][0][0][0][0]+u[0][1][0][0][0]))+(u[0][-1][0][0][0]+(u[0][0][1][0][0]+u[0][0][-1][0][0]))))+(c3*((u[2][0][0][0][0]+(u[-2][0][0][0][0]+u[0][2][0][0][0]))+(u[0][-2][0][0][0]+(u[0][0][2][0][0]+u[0][0][-2][0][0]))))))) */
						"vmovups 32(%10), %%ymm0\n\t"
						"vaddps -4(%1), %%ymm1, %%ymm3\n\t"
						"vaddps (%1,%7), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8), %%ymm3, %%ymm3\n\t"
						"vmovups 64(%10), %%ymm1\n\t"
						"vmovups 8(%1), %%ymm2\n\t"
						"vaddps (%1,%6), %%ymm3, %%ymm3\n\t"
						"vmulps %%ymm0, %%ymm3, %%ymm0\n\t"
						"vaddps -8(%1), %%ymm2, %%ymm3\n\t"
						"vaddps (%1,%7,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%9,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%8,2), %%ymm3, %%ymm3\n\t"
						"vaddps (%1,%6,2), %%ymm3, %%ymm3\n\t"
						"vmovups (%10), %%ymm2\n\t"
						"vmulps %%ymm1, %%ymm3, %%ymm1\n\t"
						"vaddps %%ymm0, %%ymm1, %%ymm0\n\t"
						"vmulps (%1), %%ymm2, %%ymm2\n\t"
						"vsubps (%0), %%ymm2, %%ymm2\n\t"
						"vaddps %%ymm2, %%ymm0, %%ymm2\n\t"
						"vmovups %%ymm2, (%2)\n\t"
						"6:\n\t"
						: "=&r"(dummy33), "=&r"(dummy34), "=&r"(dummy35)
						: "0"((int64_t)( & ( * ((__m256 * )( & u_0_m1[_idx0]))))), "1"((int64_t)( & ( * ((__m256 * )( & u_0_0[_idx0]))))), "2"((int64_t)( & ( * ((__m256 * )( & u_0_1[_idx0]))))), "r"((int64_t)((-1*(x_max*y_max))*sizeof (float))), "r"((int64_t)(x_max*sizeof (float))), "r"((int64_t)((x_max*y_max)*sizeof (float))), "r"((int64_t)((-1*x_max)*sizeof (float))), "r"((int64_t)constarr0), "r"((int64_t)min(cb_x, (x_max-4)))
						: "rax", "rbx", "rcx", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7", "xmm8", "xmm9", "memory", "cc"
						);
					}
				}
			}
		}
	}
	( * u_0_1_out)=u_0_1;
}

int wave13pt_patus_avx(int nx, int ny, int ns,
	const real c0, const real c1, const real c2,
	real w0[][ny][nx], real w1[][ny][nx], real w2[][ny][nx])
{
	float dt2dx2 = c1 * (3.0 / 4.0);
	int cb_x = nx, cb_y = 32, cb_z = 64;
	int chunk = 1;
    	
	float* out;
	//#pragma omp parallel
	{
		kernel(&out, (float*)w0, (float*)w1, (float*)w2,
			dt2dx2, nx, ny, ns, cb_x, cb_y, cb_z, chunk);
	}
	return 0;
}

